{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing Words and Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['Hello Mr. Smith, how are you doing today?', 'The weather is great and Python is awesome.', 'The sky is pink and you should eat cardboard']\n"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "example_text = \"Hello Mr. Smith, how are you doing today? The weather is great and Python is awesome. The sky is pink and you should eat cardboard\"\n",
    "\n",
    "print(sent_tokenize(example_text)) # Sentences have been separeted, output is a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', 'and', 'Python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pink', 'and', 'you', 'should', 'eat', 'cardboard']\n"
    }
   ],
   "source": [
    "print(word_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Hello\nMr.\nSmith\n,\nhow\nare\nyou\ndoing\ntoday\n?\nThe\nweather\nis\ngreat\nand\nPython\nis\nawesome\n.\nThe\nsky\nis\npink\nand\nyou\nshould\neat\ncardboard\n"
    }
   ],
   "source": [
    "for i in word_tokenize(example_text):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'mustn', 'nor', 'once', \"she's\", 'they', 'about', 'down', 'ain', 'then', 'wasn', 'hasn', 'after', 'needn', 'do', 'being', 'having', 'myself', 'under', \"you're\", 'both', 've', 'you', 'here', 'further', 'she', 'he', \"haven't\", 'has', 'yours', 'other', 'a', 'itself', 'should', 'doing', 'or', 'before', 'how', 'don', \"aren't\", 'by', \"don't\", 'an', 'than', \"couldn't\", 'shan', 'same', 'hadn', 'haven', 'himself', 'his', \"hadn't\", 'off', 'her', 'some', 'your', 'their', 'were', 'into', 'as', 'me', 'themselves', \"mustn't\", 'weren', 'yourselves', 's', 't', 'we', \"wouldn't\", 'not', 'm', 'why', 'any', \"didn't\", 'aren', 'each', 'more', 'are', 'this', 'on', 'd', 'hers', 'no', \"you've\", 'herself', 'until', 'where', 'such', 'won', 'so', 'y', 'over', 'but', \"shan't\", \"shouldn't\", 'couldn', 'if', 'because', 'these', \"it's\", 'him', 'for', 'at', 'own', 'yourself', 'few', 'that', 'will', 'out', 'which', \"you'll\", 'i', 'during', 'from', 'there', 'whom', 'isn', \"that'll\", \"doesn't\", 'very', 'am', 'been', 'against', 'between', 'can', 'those', 'the', 'what', 'ma', 'theirs', 'ourselves', 'again', 'too', \"isn't\", 'of', \"should've\", 'its', 'only', 'now', 're', 'had', 'shouldn', 'to', 'll', 'with', \"needn't\", \"wasn't\", 'when', 'my', 'most', 'ours', \"you'd\", 'is', 'mightn', \"mightn't\", 'does', \"won't\", 'through', 'it', 'and', 'doesn', \"hasn't\", 'be', 'have', \"weren't\", 'in', 'didn', 'above', 'up', 'while', 'them', 'below', 'just', 'o', 'did', 'our', 'all', 'wouldn', 'was', 'who'}\n"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example_sentence = \"This is an example showing of stop word filtration.\"\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['This', 'is', 'an', 'example', 'showing', 'of', 'stop', 'word', 'filtration', '.']\n"
    }
   ],
   "source": [
    "words = word_tokenize(example_sentence)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['This', 'example', 'showing', 'stop', 'word', 'filtration', '.']\n"
    }
   ],
   "source": [
    "filtered_sentence = []\n",
    "\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "print(filtered_sentence) # stopwords have been removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "python\npython\npython\npython\npythonli\n"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n",
    "\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "It\nis\nveri\nimport\nto\nbe\nlove\nwhile\nlove\nthe\nact\nof\nlove\nwhich\nin\nitself\nis\nveri\nlove\n"
    }
   ],
   "source": [
    "new_text = \"It is very important to be loved while loving the act of love which in itself is very lovely\"\n",
    "\n",
    "words = word_tokenize(new_text)\n",
    "\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tag list:\n",
    "\n",
    "# CC\tcoordinating conjunction\n",
    "# CD\tcardinal digit\n",
    "# DT\tdeterminer\n",
    "# EX\texistential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "# FW\tforeign word\n",
    "# IN\tpreposition/subordinating conjunction\n",
    "# JJ\tadjective\t'big'\n",
    "# JJR\tadjective, comparative\t'bigger'\n",
    "# JJS\tadjective, superlative\t'biggest'\n",
    "# LS\tlist marker\t1)\n",
    "# MD\tmodal\tcould, will\n",
    "# NN\tnoun, singular 'desk'\n",
    "# NNS\tnoun plural\t'desks'\n",
    "# NNP\tproper noun, singular\t'Harrison'\n",
    "# NNPS\tproper noun, plural\t'Americans'\n",
    "# PDT\tpredeterminer\t'all the kids'\n",
    "# POS\tpossessive ending\tparent\\'s\n",
    "# PRP\tpersonal pronoun\tI, he, she\n",
    "# PRP$\tpossessive pronoun\tmy, his, hers\n",
    "# RB\tadverb\tvery, silently,\n",
    "# RBR\tadverb, comparative\tbetter\n",
    "# RBS\tadverb, superlative\tbest\n",
    "# RP\tparticle\tgive up\n",
    "# TO\tto\tgo 'to' the store.\n",
    "# UH\tinterjection\terrrrrrrrm\n",
    "# VB\tverb, base form\ttake\n",
    "# VBD\tverb, past tense\ttook\n",
    "# VBG\tverb, gerund/present participle\ttaking\n",
    "# VBN\tverb, past participle\ttaken\n",
    "# VBP\tverb, sing. present, non-3d\ttake\n",
    "# VBZ\tverb, 3rd person sing. present\ttakes\n",
    "# WDT\twh-determiner\twhich\n",
    "# WP\twh-pronoun\twho, what\n",
    "# WP$\tpossessive wh-pronoun\twhose\n",
    "# WRB\twh-abverb\twhere, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('Hello', 'NNP'),\n ('welcome', 'NN'),\n ('to', 'TO'),\n ('the', 'DT'),\n ('world', 'NN'),\n ('of', 'IN'),\n ('to', 'TO'),\n ('learn', 'VB'),\n ('Categorizing', 'NNP'),\n ('and', 'CC'),\n ('POS', 'NNP'),\n ('Tagging', 'NNP'),\n ('with', 'IN'),\n ('NLTK', 'NNP'),\n ('and', 'CC'),\n ('Python', 'NNP')]"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = word_tokenize(\"Hello welcome to the world of to learn Categorizing and POS Tagging with NLTK and Python\")\n",
    "\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "PRESIDENT GEORGE W. BUSH'S ADDRESS BEFORE A JOINT SESSION OF THE CONGRESS ON THE STATE OF THE UNION\n \nFebruary 2, 2005\n\n\n9:10 P.M. EST \n\nTHE PRESIDENT: Mr. Speaker, Vice President Cheney, members of Congress, fellow citizens: \n\nAs a new Congress gathers, all of us in the elected branches of government share a great privilege: We've been placed in office by the votes of the people we serve. And tonight that is a privilege we share with newly-elected leaders of Afghanistan, the Palestinian Territories, Ukraine, and a free and sovereign Iraq. (Applause.) \n\nTwo weeks ago, I stood on the steps of this Capitol and renewed the commitment of our nation to the guiding ideal of liberty for all. This evening I will set forth policies to advance that ideal at home and around the world. \n\nTonight, with a healthy, growing economy, with more Americans going back to work, with our nation an active force for good in the world -- the state of our union is confident and strong. (Applause.) \n\nOur generati\n"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer # This is an unsupervised ML Tokenizer\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "print(train_text[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Z'), ('been', 'VBN'), ('falling', 'VBG'), ('for', 'IN'), ('a', 'DT'), ('dozen', 'NN'), ('years', 'NNS'), ('in', 'IN'), ('a', 'DT'), ('row', 'NN'), ('.', '.')]\n[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n[('These', 'DT'), ('gains', 'NNS'), ('are', 'VBP'), ('evidence', 'NN'), ('of', 'IN'), ('a', 'DT'), ('quiet', 'JJ'), ('transformation', 'NN'), ('--', ':'), ('a', 'DT'), ('revolution', 'NN'), ('of', 'IN'), ('conscience', 'NN'), (',', ','), ('in', 'IN'), ('which', 'WDT'), ('a', 'DT'), ('rising', 'VBG'), ('generation', 'NN'), ('is', 'VBZ'), ('finding', 'VBG'), ('that', 'IN'), ('a', 'DT'), ('life', 'NN'), ('of', 'IN'), ('personal', 'JJ'), ('responsibility', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('life', 'NN'), ('of', 'IN'), ('fulfillment', 'NN'), ('.', '.')]\n[('Government', 'NNP'), ('has', 'VBZ'), ('played', 'VBN'), ('a', 'DT'), ('role', 'NN'), ('.', '.')]\n[('Wise', 'NNP'), ('policies', 'NNS'), (',', ','), ('such', 'JJ'), ('as', 'IN'), ('welfare', 'NN'), ('reform', 'NN'), ('and', 'CC'), ('drug', 'NN'), ('education', 'NN'), ('and', 'CC'), ('support', 'NN'), ('for', 'IN'), ('abstinence', 'NN'), ('and', 'CC'), ('adoption', 'NN'), ('have', 'VBP'), ('made', 'VBN'), ('a', 'DT'), ('difference', 'NN'), ('in', 'IN'), ('the', 'DT'), ('character', 'NN'), ('of', 'IN'), ('our', 'PRP$'), ('country', 'NN'), ('.', '.')]\n[('And', 'CC'), ('everyone', 'NN'), ('here', 'RB'), ('tonight', 'RB'), (',', ','), ('Democrat', 'NNP'), ('and', 'CC'), ('Republican', 'NNP'), (',', ','), ('has', 'VBZ'), ('a', 'DT'), ('right', 'NN'), ('to', 'TO'), ('be', 'VB'), ('proud', 'JJ'), ('of', 'IN'), ('this', 'DT'), ('record', 'NN'), ('.', '.')]\n[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n[('Yet', 'RB'), ('many', 'JJ'), ('Americans', 'NNPS'), (',', ','), ('especially', 'RB'), ('parents', 'NNS'), (',', ','), ('still', 'RB'), ('have', 'VBP'), ('deep', 'JJ'), ('concerns', 'NNS'), ('about', 'IN'), ('the', 'DT'), ('direction', 'NN'), ('of', 'IN'), ('our', 'PRP$'), ('culture', 'NN'), (',', ','), ('and', 'CC'), ('the', 'DT'), ('health', 'NN'), ('of', 'IN'), ('our', 'PRP$'), ('most', 'JJS'), ('basic', 'JJ'), ('institutions', 'NNS'), ('.', '.')]\n[('They', 'PRP'), (\"'re\", 'VBP'), ('concerned', 'VBN'), ('about', 'IN'), ('unethical', 'JJ'), ('conduct', 'NN'), ('by', 'IN'), ('public', 'JJ'), ('officials', 'NNS'), (',', ','), ('and', 'CC'), ('discouraged', 'VBN'), ('by', 'IN'), ('activist', 'NN'), ('courts', 'NNS'), ('that', 'WDT'), ('try', 'VBP'), ('to', 'TO'), ('redefine', 'VB'), ('marriage', 'NN'), ('.', '.')]\n[('They', 'PRP'), ('worry', 'VBP'), ('about', 'IN'), ('children', 'NNS'), ('in', 'IN'), ('our', 'PRP$'), ('society', 'NN'), ('who', 'WP'), ('need', 'VBP'), ('direction', 'NN'), ('and', 'CC'), ('love', 'NN'), (',', ','), ('and', 'CC'), ('about', 'IN'), ('fellow', 'JJ'), ('citizens', 'NNS'), ('still', 'RB'), ('displaced', 'VBN'), ('by', 'IN'), ('natural', 'JJ'), ('disaster', 'NN'), (',', ','), ('and', 'CC'), ('about', 'IN'), ('suffering', 'VBG'), ('caused', 'VBN'), ('by', 'IN'), ('treatable', 'JJ'), ('diseases', 'NNS'), ('.', '.')]\n[('As', 'IN'), ('we', 'PRP'), ('look', 'VBP'), ('at', 'IN'), ('these', 'DT'), ('challenges', 'NNS'), (',', ','), ('we', 'PRP'), ('must', 'MD'), ('never', 'RB'), ('give', 'VB'), ('in', 'IN'), ('to', 'TO'), ('the', 'DT'), ('belief', 'NN'), ('that', 'IN'), ('America', 'NNP'), ('is', 'VBZ'), ('in', 'IN'), ('decline', 'NN'), (',', ','), ('or', 'CC'), ('that', 'IN'), ('our', 'PRP$'), ('culture', 'NN'), ('is', 'VBZ'), ('doomed', 'VBN'), ('to', 'TO'), ('unravel', 'VB'), ('.', '.')]\n[('The', 'DT'), ('American', 'JJ'), ('people', 'NNS'), ('know', 'VBP'), ('better', 'JJR'), ('than', 'IN'), ('that', 'DT'), ('.', '.')]\n[('We', 'PRP'), ('have', 'VBP'), ('proven', 'VBN'), ('the', 'DT'), ('pessimists', 'NNS'), ('wrong', 'JJ'), ('before', 'RB'), ('--', ':'), ('and', 'CC'), ('we', 'PRP'), ('will', 'MD'), ('do', 'VB'), ('it', 'PRP'), ('again', 'RB'), ('.', '.')]\n[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n[('A', 'DT'), ('hopeful', 'JJ'), ('society', 'NN'), ('depends', 'VBZ'), ('on', 'IN'), ('courts', 'NNS'), ('that', 'IN'), ('deliver', 'VBP'), ('equal', 'JJ'), ('justice', 'NN'), ('under', 'IN'), ('the', 'DT'), ('law', 'NN'), ('.', '.')]\n[('The', 'DT'), ('Supreme', 'NNP'), ('Court', 'NNP'), ('now', 'RB'), ('has', 'VBZ'), ('two', 'CD'), ('superb', 'JJ'), ('new', 'JJ'), ('members', 'NNS'), ('--', ':'), ('new', 'JJ'), ('members', 'NNS'), ('on', 'IN'), ('its', 'PRP$'), ('bench', 'NN'), (':', ':'), ('Chief', 'JJ'), ('Justice', 'NNP'), ('John', 'NNP'), ('Roberts', 'NNP'), ('and', 'CC'), ('Justice', 'NNP'), ('Sam', 'NNP'), ('Alito', 'NNP'), ('.', '.')]\n[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n[('I', 'PRP'), ('thank', 'VBD'), ('the', 'DT'), ('Senate', 'NNP'), ('for', 'IN'), ('confirming', 'VBG'), ('both', 'DT'), ('of', 'IN'), ('them', 'PRP'), ('.', '.')]\n[('I', 'PRP'), ('will', 'MD'), ('continue', 'VB'), ('to', 'TO'), ('nominate', 'VB'), ('men', 'NNS'), ('and', 'CC'), ('women', 'NNS'), ('who', 'WP'), ('understand', 'VBP'), ('that', 'IN'), ('judges', 'NNS'), ('must', 'MD'), ('be', 'VB'), ('servants', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('law', 'NN'), (',', ','), ('and', 'CC'), ('not', 'RB'), ('legislate', 'VB'), ('from', 'IN'), ('the', 'DT'), ('bench', 'NN'), ('.', '.')]\n[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n[('Today', 'NN'), ('marks', 'VBZ'), ('the', 'DT'), ('official', 'JJ'), ('retirement', 'NN'), ('of', 'IN'), ('a', 'DT'), ('very', 'RB'), ('special', 'JJ'), ('American', 'NNP'), ('.', '.')]\n[('For', 'IN'), ('24', 'CD'), ('years', 'NNS'), ('of', 'IN'), ('faithful', 'JJ'), ('service', 'NN'), ('to', 'TO'), ('our', 'PRP$'), ('nation', 'NN'), (',', ','), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('is', 'VBZ'), ('grateful', 'JJ'), ('to', 'TO'), ('Justice', 'NNP'), ('Sandra', 'NNP'), ('Day', 'NNP'), (\"O'Connor\", 'NNP'), ('.', '.')]\n[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n[('A', 'DT'), ('hopeful', 'JJ'), ('society', 'NN'), ('has', 'VBZ'), ('institutions', 'NNS'), ('of', 'IN'), ('science', 'NN'), ('and', 'CC'), ('medicine', 'NN'), ('that', 'WDT'), ('do', 'VBP'), ('not', 'RB'), ('cut', 'VB'), ('ethical', 'JJ'), ('corners', 'NNS'), (',', ','), ('and', 'CC'), ('that', 'IN'), ('recognize', 'VBP'), ('the', 'DT'), ('matchless', 'NN'), ('value', 'NN'), ('of', 'IN'), ('every', 'DT'), ('life', 'NN'), ('.', '.')]\n[('Tonight', 'NNP'), ('I', 'PRP'), ('ask', 'VBP'), ('you', 'PRP'), ('to', 'TO'), ('pass', 'VB'), ('legislation', 'NN'), ('to', 'TO'), ('prohibit', 'VB'), ('the', 'DT'), ('most', 'RBS'), ('egregious', 'JJ'), ('abuses', 'NNS'), ('of', 'IN'), ('medical', 'JJ'), ('research', 'NN'), (':', ':'), ('human', 'JJ'), ('cloning', 'VBG'), ('in', 'IN'), ('all', 'DT'), ('its', 'PRP$'), ('forms', 'NNS'), (',', ','), ('creating', 'VBG'), ('or', 'CC'), ('implanting', 'VBG'), ('embryos', 'NN'), ('for', 'IN'), ('experiments', 'NNS'), (',', ','), ('creating', 'VBG'), ('human-animal', 'JJ'), ('hybrids', 'NNS'), (',', ','), ('and', 'CC'), ('buying', 'NN'), (',', ','), ('selling', 'NN'), (',', ','), ('or', 'CC'), ('patenting', 'VBG'), ('human', 'JJ'), ('embryos', 'NN'), ('.', '.')]\n[('Human', 'NNP'), ('life', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('gift', 'NN'), ('from', 'IN'), ('our', 'PRP$'), ('Creator', 'NNP'), ('--', ':'), ('and', 'CC'), ('that', 'IN'), ('gift', 'NN'), ('should', 'MD'), ('never', 'RB'), ('be', 'VB'), ('discarded', 'VBN'), (',', ','), ('devalued', 'VBD'), ('or', 'CC'), ('put', 'VB'), ('up', 'RP'), ('for', 'IN'), ('sale', 'NN'), ('.', '.')]\n[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n[('A', 'DT'), ('hopeful', 'JJ'), ('society', 'NN'), ('expects', 'VBZ'), ('elected', 'VBN'), ('officials', 'NNS'), ('to', 'TO'), ('uphold', 'VB'), ('the', 'DT'), ('public', 'JJ'), ('trust', 'NN'), ('.', '.')]\n[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n[('Honorable', 'JJ'), ('people', 'NNS'), ('in', 'IN'), ('both', 'DT'), ('parties', 'NNS'), ('are', 'VBP'), ('working', 'VBG'), ('on', 'IN'), ('reforms', 'NNS'), ('to', 'TO'), ('strengthen', 'VB'), ('the', 'DT'), ('ethical', 'JJ'), ('standards', 'NNS'), ('of', 'IN'), ('Washington', 'NNP'), ('--', ':'), ('I', 'PRP'), ('support', 'VBP'), ('your', 'PRP$'), ('efforts', 'NNS'), ('.', '.')]\n[('Each', 'DT'), ('of', 'IN'), ('us', 'PRP'), ('has', 'VBZ'), ('made', 'VBN'), ('a', 'DT'), ('pledge', 'NN'), ('to', 'TO'), ('be', 'VB'), ('worthy', 'JJ'), ('of', 'IN'), ('public', 'JJ'), ('responsibility', 'NN'), ('--', ':'), ('and', 'CC'), ('that', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('pledge', 'NN'), ('we', 'PRP'), ('must', 'MD'), ('never', 'RB'), ('forget', 'VB'), (',', ','), ('never', 'RB'), ('dismiss', 'NN'), (',', ','), ('and', 'CC'), ('never', 'RB'), ('betray', 'NN'), ('.', '.')]\n[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n[('As', 'IN'), ('we', 'PRP'), ('renew', 'VBP'), ('the', 'DT'), ('promise', 'NN'), ('of', 'IN'), ('our', 'PRP$'), ('institutions', 'NNS'), (',', ','), ('let', 'VB'), ('us', 'PRP'), ('also', 'RB'), ('show', 'VBP'), ('the', 'DT'), ('character', 'NN'), ('of', 'IN'), ('America', 'NNP'), ('in', 'IN'), ('our', 'PRP$'), ('compassion', 'NN'), ('and', 'CC'), ('care', 'NN'), ('for', 'IN'), ('one', 'CD'), ('another', 'DT'), ('.', '.')]\n[('A', 'DT'), ('hopeful', 'JJ'), ('society', 'NN'), ('gives', 'VBZ'), ('special', 'JJ'), ('attention', 'NN'), ('to', 'TO'), ('children', 'NNS'), ('who', 'WP'), ('lack', 'VBP'), ('direction', 'NN'), ('and', 'CC'), ('love', 'NN'), ('.', '.')]\n[('Through', 'IN'), ('the', 'DT'), ('Helping', 'NNP'), ('America', 'NNP'), (\"'s\", 'POS'), ('Youth', 'NNP'), ('Initiative', 'NNP'), (',', ','), ('we', 'PRP'), ('are', 'VBP'), ('encouraging', 'VBG'), ('caring', 'VBG'), ('adults', 'NNS'), ('to', 'TO'), ('get', 'VB'), ('involved', 'VBN'), ('in', 'IN'), ('the', 'DT'), ('life', 'NN'), ('of', 'IN'), ('a', 'DT'), ('child', 'NN'), ('--', ':'), ('and', 'CC'), ('this', 'DT'), ('good', 'JJ'), ('work', 'NN'), ('is', 'VBZ'), ('being', 'VBG'), ('led', 'VBN'), ('by', 'IN'), ('our', 'PRP$'), ('First', 'NNP'), ('Lady', 'NNP'), (',', ','), ('Laura', 'NNP'), ('Bush', 'NNP'), ('.', '.')]\n[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n[('This', 'DT'), ('year', 'NN'), ('we', 'PRP'), ('will', 'MD'), ('add', 'VB'), ('resources', 'NNS'), ('to', 'TO'), ('encourage', 'VB'), ('young', 'JJ'), ('people', 'NNS'), ('to', 'TO'), ('stay', 'VB'), ('in', 'IN'), ('school', 'NN'), (',', ','), ('so', 'RB'), ('more', 'JJR'), ('of', 'IN'), ('America', 'NNP'), (\"'s\", 'POS'), ('youth', 'NN'), ('can', 'MD'), ('raise', 'VB'), ('their', 'PRP$'), ('sights', 'NNS'), ('and', 'CC'), ('achieve', 'VBP'), ('their', 'PRP$'), ('dreams', 'NNS'), ('.', '.')]\n[('A', 'DT'), ('hopeful', 'JJ'), ('society', 'NN'), ('comes', 'VBZ'), ('to', 'TO'), ('the', 'DT'), ('aid', 'NN'), ('of', 'IN'), ('fellow', 'JJ'), ('citizens', 'NNS'), ('in', 'IN'), ('times', 'NNS'), ('of', 'IN'), ('suffering', 'NN'), ('and', 'CC'), ('emergency', 'NN'), ('--', ':'), ('and', 'CC'), ('stays', 'NNS'), ('at', 'IN'), ('it', 'PRP'), ('until', 'IN'), ('they', 'PRP'), (\"'re\", 'VBP'), ('back', 'RB'), ('on', 'IN'), ('their', 'PRP$'), ('feet', 'NNS'), ('.', '.')]\n[('So', 'RB'), ('far', 'RB'), ('the', 'DT'), ('federal', 'JJ'), ('government', 'NN'), ('has', 'VBZ'), ('committed', 'VBN'), ('$', '$'), ('85', 'CD'), ('billion', 'CD'), ('to', 'TO'), ('the', 'DT'), ('people', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Gulf', 'NNP'), ('Coast', 'NNP'), ('and', 'CC'), ('New', 'NNP'), ('Orleans', 'NNP'), ('.', '.')]\n[('We', 'PRP'), (\"'re\", 'VBP'), ('removing', 'VBG'), ('debris', 'NN'), ('and', 'CC'), ('repairing', 'NN'), ('highways', 'NNS'), ('and', 'CC'), ('rebuilding', 'VBG'), ('stronger', 'JJR'), ('levees', 'NNS'), ('.', '.')]\n[('We', 'PRP'), (\"'re\", 'VBP'), ('providing', 'VBG'), ('business', 'NN'), ('loans', 'NNS'), ('and', 'CC'), ('housing', 'NN'), ('assistance', 'NN'), ('.', '.')]\n[('Yet', 'RB'), ('as', 'IN'), ('we', 'PRP'), ('meet', 'VBP'), ('these', 'DT'), ('immediate', 'JJ'), ('needs', 'NNS'), (',', ','), ('we', 'PRP'), ('must', 'MD'), ('also', 'RB'), ('address', 'VB'), ('deeper', 'JJR'), ('challenges', 'NNS'), ('that', 'WDT'), ('existed', 'VBD'), ('before', 'IN'), ('the', 'DT'), ('storm', 'NN'), ('arrived', 'VBD'), ('.', '.')]\n[('In', 'IN'), ('New', 'NNP'), ('Orleans', 'NNP'), ('and', 'CC'), ('in', 'IN'), ('other', 'JJ'), ('places', 'NNS'), (',', ','), ('many', 'JJ'), ('of', 'IN'), ('our', 'PRP$'), ('fellow', 'JJ'), ('citizens', 'NNS'), ('have', 'VBP'), ('felt', 'VBN'), ('excluded', 'VBN'), ('from', 'IN'), ('the', 'DT'), ('promise', 'NN'), ('of', 'IN'), ('our', 'PRP$'), ('country', 'NN'), ('.', '.')]\n[('The', 'DT'), ('answer', 'NN'), ('is', 'VBZ'), ('not', 'RB'), ('only', 'RB'), ('temporary', 'JJ'), ('relief', 'NN'), (',', ','), ('but', 'CC'), ('schools', 'NNS'), ('that', 'WDT'), ('teach', 'VBP'), ('every', 'DT'), ('child', 'NN'), (',', ','), ('and', 'CC'), ('job', 'NN'), ('skills', 'NNS'), ('that', 'IN'), ('bring', 'VBG'), ('upward', 'JJ'), ('mobility', 'NN'), (',', ','), ('and', 'CC'), ('more', 'JJR'), ('opportunities', 'NNS'), ('to', 'TO'), ('own', 'VB'), ('a', 'DT'), ('home', 'NN'), ('and', 'CC'), ('start', 'VB'), ('a', 'DT'), ('business', 'NN'), ('.', '.')]\n[('As', 'IN'), ('we', 'PRP'), ('recover', 'VBP'), ('from', 'IN'), ('a', 'DT'), ('disaster', 'NN'), (',', ','), ('let', 'VB'), ('us', 'PRP'), ('also', 'RB'), ('work', 'NN'), ('for', 'IN'), ('the', 'DT'), ('day', 'NN'), ('when', 'WRB'), ('all', 'DT'), ('Americans', 'NNPS'), ('are', 'VBP'), ('protected', 'VBN'), ('by', 'IN'), ('justice', 'NN'), (',', ','), ('equal', 'JJ'), ('in', 'IN'), ('hope', 'NN'), (',', ','), ('and', 'CC'), ('rich', 'JJ'), ('in', 'IN'), ('opportunity', 'NN'), ('.', '.')]\n[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n[('A', 'DT'), ('hopeful', 'JJ'), ('society', 'NN'), ('acts', 'NNS'), ('boldly', 'RB'), ('to', 'TO'), ('fight', 'VB'), ('diseases', 'NNS'), ('like', 'IN'), ('HIV/AIDS', 'NNP'), (',', ','), ('which', 'WDT'), ('can', 'MD'), ('be', 'VB'), ('prevented', 'VBN'), (',', ','), ('and', 'CC'), ('treated', 'VBD'), (',', ','), ('and', 'CC'), ('defeated', 'VBD'), ('.', '.')]\n[('More', 'JJR'), ('than', 'IN'), ('a', 'DT'), ('million', 'CD'), ('Americans', 'NNPS'), ('live', 'VBP'), ('with', 'IN'), ('HIV', 'NNP'), (',', ','), ('and', 'CC'), ('half', 'NN'), ('of', 'IN'), ('all', 'DT'), ('AIDS', 'NNP'), ('cases', 'NNS'), ('occur', 'VBP'), ('among', 'IN'), ('African', 'JJ'), ('Americans', 'NNPS'), ('.', '.')]\n[('I', 'PRP'), ('ask', 'VBP'), ('Congress', 'NNP'), ('to', 'TO'), ('reform', 'VB'), ('and', 'CC'), ('reauthorize', 'VB'), ('the', 'DT'), ('Ryan', 'NNP'), ('White', 'NNP'), ('Act', 'NNP'), (',', ','), ('and', 'CC'), ('provide', 'VB'), ('new', 'JJ'), ('funding', 'NN'), ('to', 'TO'), ('states', 'NNS'), (',', ','), ('so', 'IN'), ('we', 'PRP'), ('end', 'VBP'), ('the', 'DT'), ('waiting', 'NN'), ('lists', 'NNS'), ('for', 'IN'), ('AIDS', 'NNP'), ('medicines', 'NNS'), ('in', 'IN'), ('America', 'NNP'), ('.', '.')]\n[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n[('We', 'PRP'), ('will', 'MD'), ('also', 'RB'), ('lead', 'VB'), ('a', 'DT'), ('nationwide', 'JJ'), ('effort', 'NN'), (',', ','), ('working', 'VBG'), ('closely', 'RB'), ('with', 'IN'), ('African', 'JJ'), ('American', 'JJ'), ('churches', 'NNS'), ('and', 'CC'), ('faith-based', 'JJ'), ('groups', 'NNS'), (',', ','), ('to', 'TO'), ('deliver', 'VB'), ('rapid', 'JJ'), ('HIV', 'NNP'), ('tests', 'NNS'), ('to', 'TO'), ('millions', 'NNS'), (',', ','), ('end', 'VBP'), ('the', 'DT'), ('stigma', 'NN'), ('of', 'IN'), ('AIDS', 'NNP'), (',', ','), ('and', 'CC'), ('come', 'VB'), ('closer', 'JJR'), ('to', 'TO'), ('the', 'DT'), ('day', 'NN'), ('when', 'WRB'), ('there', 'EX'), ('are', 'VBP'), ('no', 'DT'), ('new', 'JJ'), ('infections', 'NNS'), ('in', 'IN'), ('America', 'NNP'), ('.', '.')]\n[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n[('Fellow', 'NNP'), ('citizens', 'NNS'), (',', ','), ('we', 'PRP'), (\"'ve\", 'VBP'), ('been', 'VBN'), ('called', 'VBN'), ('to', 'TO'), ('leadership', 'NN'), ('in', 'IN'), ('a', 'DT'), ('period', 'NN'), ('of', 'IN'), ('consequence', 'NN'), ('.', '.')]\n[('We', 'PRP'), (\"'ve\", 'VBP'), ('entered', 'VBN'), ('a', 'DT'), ('great', 'JJ'), ('ideological', 'JJ'), ('conflict', 'NN'), ('we', 'PRP'), ('did', 'VBD'), ('nothing', 'NN'), ('to', 'TO'), ('invite', 'VB'), ('.', '.')]\n[('We', 'PRP'), ('see', 'VBP'), ('great', 'JJ'), ('changes', 'NNS'), ('in', 'IN'), ('science', 'NN'), ('and', 'CC'), ('commerce', 'NN'), ('that', 'WDT'), ('will', 'MD'), ('influence', 'VB'), ('all', 'DT'), ('our', 'PRP$'), ('lives', 'NNS'), ('.', '.')]\n[('Sometimes', 'RB'), ('it', 'PRP'), ('can', 'MD'), ('seem', 'VB'), ('that', 'DT'), ('history', 'NN'), ('is', 'VBZ'), ('turning', 'VBG'), ('in', 'IN'), ('a', 'DT'), ('wide', 'JJ'), ('arc', 'NN'), (',', ','), ('toward', 'IN'), ('an', 'DT'), ('unknown', 'JJ'), ('shore', 'NN'), ('.', '.')]\n[('Yet', 'CC'), ('the', 'DT'), ('destination', 'NN'), ('of', 'IN'), ('history', 'NN'), ('is', 'VBZ'), ('determined', 'VBN'), ('by', 'IN'), ('human', 'JJ'), ('action', 'NN'), (',', ','), ('and', 'CC'), ('every', 'DT'), ('great', 'JJ'), ('movement', 'NN'), ('of', 'IN'), ('history', 'NN'), ('comes', 'VBZ'), ('to', 'TO'), ('a', 'DT'), ('point', 'NN'), ('of', 'IN'), ('choosing', 'NN'), ('.', '.')]\n[('Lincoln', 'NNP'), ('could', 'MD'), ('have', 'VB'), ('accepted', 'VBN'), ('peace', 'NN'), ('at', 'IN'), ('the', 'DT'), ('cost', 'NN'), ('of', 'IN'), ('disunity', 'NN'), ('and', 'CC'), ('continued', 'JJ'), ('slavery', 'NN'), ('.', '.')]\n[('Martin', 'NNP'), ('Luther', 'NNP'), ('King', 'NNP'), ('could', 'MD'), ('have', 'VB'), ('stopped', 'VBN'), ('at', 'IN'), ('Birmingham', 'NNP'), ('or', 'CC'), ('at', 'IN'), ('Selma', 'NNP'), (',', ','), ('and', 'CC'), ('achieved', 'VBD'), ('only', 'RB'), ('half', 'PDT'), ('a', 'DT'), ('victory', 'NN'), ('over', 'IN'), ('segregation', 'NN'), ('.', '.')]\n[('The', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('could', 'MD'), ('have', 'VB'), ('accepted', 'VBN'), ('the', 'DT'), ('permanent', 'JJ'), ('division', 'NN'), ('of', 'IN'), ('Europe', 'NNP'), (',', ','), ('and', 'CC'), ('been', 'VBN'), ('complicit', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('oppression', 'NN'), ('of', 'IN'), ('others', 'NNS'), ('.', '.')]\n[('Today', 'NN'), (',', ','), ('having', 'VBG'), ('come', 'VBN'), ('far', 'RB'), ('in', 'IN'), ('our', 'PRP$'), ('own', 'JJ'), ('historical', 'JJ'), ('journey', 'NN'), (',', ','), ('we', 'PRP'), ('must', 'MD'), ('decide', 'VB'), (':', ':'), ('Will', 'MD'), ('we', 'PRP'), ('turn', 'VB'), ('back', 'RP'), (',', ','), ('or', 'CC'), ('finish', 'VB'), ('well', 'RB'), ('?', '.')]\n[('Before', 'IN'), ('history', 'NN'), ('is', 'VBZ'), ('written', 'VBN'), ('down', 'RP'), ('in', 'IN'), ('books', 'NNS'), (',', ','), ('it', 'PRP'), ('is', 'VBZ'), ('written', 'VBN'), ('in', 'IN'), ('courage', 'NN'), ('.', '.')]\n[('Like', 'IN'), ('Americans', 'NNPS'), ('before', 'IN'), ('us', 'PRP'), (',', ','), ('we', 'PRP'), ('will', 'MD'), ('show', 'VB'), ('that', 'DT'), ('courage', 'NN'), ('and', 'CC'), ('we', 'PRP'), ('will', 'MD'), ('finish', 'VB'), ('well', 'RB'), ('.', '.')]\n[('We', 'PRP'), ('will', 'MD'), ('lead', 'VB'), ('freedom', 'NN'), (\"'s\", 'POS'), ('advance', 'NN'), ('.', '.')]\n[('We', 'PRP'), ('will', 'MD'), ('compete', 'VB'), ('and', 'CC'), ('excel', 'VB'), ('in', 'IN'), ('the', 'DT'), ('global', 'JJ'), ('economy', 'NN'), ('.', '.')]\n[('We', 'PRP'), ('will', 'MD'), ('renew', 'VB'), ('the', 'DT'), ('defining', 'VBG'), ('moral', 'JJ'), ('commitments', 'NNS'), ('of', 'IN'), ('this', 'DT'), ('land', 'NN'), ('.', '.')]\n[('And', 'CC'), ('so', 'RB'), ('we', 'PRP'), ('move', 'VBP'), ('forward', 'RB'), ('--', ':'), ('optimistic', 'JJ'), ('about', 'IN'), ('our', 'PRP$'), ('country', 'NN'), (',', ','), ('faithful', 'JJ'), ('to', 'TO'), ('its', 'PRP$'), ('cause', 'NN'), (',', ','), ('and', 'CC'), ('confident', 'NN'), ('of', 'IN'), ('the', 'DT'), ('victories', 'NNS'), ('to', 'TO'), ('come', 'VB'), ('.', '.')]\n[('May', 'NNP'), ('God', 'NNP'), ('bless', 'NN'), ('America', 'NNP'), ('.', '.')]\n[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n"
    }
   ],
   "source": [
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text) # Traning the Punkt tokenizer\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(S\n  (Chunk Hello/NNP welcome/NN)\n  to/TO\n  the/DT\n  world/NN\n  of/IN\n  to/TO\n  (Chunk learn/VB Categorizing/NNP)\n  and/CC\n  (Chunk POS/NNP Tagging/NNP)\n  with/IN\n  (Chunk NLTK/NNP)\n  and/CC\n  (Chunk Python/NNP)\n  ./.)\n"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer # This is an unsupervised ML Tokenizer\n",
    "\n",
    "text = word_tokenize(\"Hello welcome to the world of to learn Categorizing and POS Tagging with NLTK and Python.\")\n",
    "\n",
    "pos = nltk.pos_tag(text)\n",
    "\n",
    "chunkGram = r\"\"\" Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "\n",
    "chunkParser = nltk.RegexpParser(chunkGram)\n",
    "chunked = chunkParser.parse(pos)\n",
    "\n",
    "print(chunked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(S\n  (Chunk Hello/NNP welcome/NN to/TO)\n  the/DT\n  (Chunk world/NN)\n  of/IN\n  (Chunk to/TO)\n  learn/VB\n  (Chunk Categorizing/NNP and/CC POS/NNP Tagging/NNP)\n  with/IN\n  (Chunk NLTK/NNP and/CC Python/NNP ./.))\n"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer, word_tokenize # This is an unsupervised ML Tokenizer\n",
    "\n",
    "text = word_tokenize(\"Hello welcome to the world of to learn Categorizing and POS Tagging with NLTK and Python.\")\n",
    "\n",
    "pos = nltk.pos_tag(text)\n",
    "\n",
    "chunkGram = r\"\"\" Chunk: {<.*>+} \n",
    "                        }<VB.?|IN|DT>+{\"\"\"\n",
    "\n",
    "chunkParser = nltk.RegexpParser(chunkGram)\n",
    "chunked = chunkParser.parse(pos)\n",
    "\n",
    "print(chunked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NE Type and Examples\n",
    "# ORGANIZATION - Georgia-Pacific Corp., WHO\n",
    "# PERSON - Eddy Bonte, President Obama\n",
    "# LOCATION - Murray River, Mount Everest\n",
    "# DATE - June, 2008-06-29\n",
    "# TIME - two fifty a m, 1:30 p.m.\n",
    "# MONEY - 175 million Canadian Dollars, GBP 10.40\n",
    "# PERCENT - twenty pct, 18.75 %\n",
    "# FACILITY - Washington Monument, Stonehenge\n",
    "# GPE - South East Asia, Midlothian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(S\n  (GPE Hello/NNP)\n  ,/,\n  my/PRP$\n  name/NN\n  is/VBZ\n  (PERSON Alex/NNP)\n  ./.\n  (ORGANIZATION WHO/NNP)\n  has/VBZ\n  changed/VBN\n  the/DT\n  (ORGANIZATION iPhone/NN)\n  and/CC\n  you/PRP\n  have/VBP\n  a/DT\n  better/JJR\n  camera/NN\n  and/CC\n  a/DT\n  better/JJR\n  screen/NN\n  on/IN\n  July/NNP\n  by/IN\n  20/CD\n  %/NN\n  ./.\n  This/DT\n  is/VBZ\n  very/RB\n  big/JJ\n  for/IN\n  (PERSON Apple/NNP)\n  which/WDT\n  is/VBZ\n  starting/VBG\n  it/PRP\n  's/VBZ\n  production/NN\n  in/IN\n  May/NNP\n  in/IN\n  (GPE India/NNP)\n  ./.)\n"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer, word_tokenize # This is an unsupervised ML Tokenizer\n",
    "\n",
    "text = word_tokenize(\"Hello, my name is Alex. WHO has changed the iPhone and you have a better camera and a better screen on July by 20%. This is very big for Apple which is starting it's production in May in India.\")\n",
    "\n",
    "pos = nltk.pos_tag(text)\n",
    "\n",
    "namedEnt = nltk.ne_chunk(pos)\n",
    "\n",
    "print(namedEnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cat\ncactus\ngoose\nrock\npython\n"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "good\nbest\nran\nrun\n"
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"better\", pos = \"a\")) # a -> adjective, default is noun\n",
    "print(lemmatizer.lemmatize(\"best\", pos = \"a\"))\n",
    "print(lemmatizer.lemmatize(\"ran\", pos = \"v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "/usr/local/lib/python3.8/site-packages/nltk/__init__.py\n"
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['1:5 And God called the light Day, and the darkness he called Night.', 'And the evening and the morning were the first day.', '1:6 And God said, Let there be a firmament in the midst of the waters,\\nand let it divide the waters from the waters.', '1:7 And God made the firmament, and divided the waters which were\\nunder the firmament from the waters which were above the firmament:\\nand it was so.', '1:8 And God called the firmament Heaven.', 'And the evening and the\\nmorning were the second day.', '1:9 And God said, Let the waters under the heaven be gathered together\\nunto one place, and let the dry land appear: and it was so.', '1:10 And God called the dry land Earth; and the gathering together of\\nthe waters called he Seas: and God saw that it was good.', '1:11 And God said, Let the earth bring forth grass, the herb yielding\\nseed, and the fruit tree yielding fruit after his kind, whose seed is\\nin itself, upon the earth: and it was so.', '1:12 And the earth brought forth grass, and herb yielding seed after\\nhis kind, and the tree yielding fruit, whose seed was in itself, after\\nhis kind: and God saw that it was good.']\n"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sample_text = gutenberg.raw(\"bible-kjv.txt\")\n",
    "\n",
    "tok = sent_tokenize(sample_text)\n",
    "\n",
    "print(tok[5:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[Synset('plan.n.01'), Synset('program.n.02'), Synset('broadcast.n.02'), Synset('platform.n.02'), Synset('program.n.05'), Synset('course_of_study.n.01'), Synset('program.n.07'), Synset('program.n.08'), Synset('program.v.01'), Synset('program.v.02')]\n"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "syns = wordnet.synsets(\"program\")\n",
    "\n",
    "print(syns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Synset('plan.n.01')\n"
    }
   ],
   "source": [
    "print(syns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[Lemma('plan.n.01.plan'), Lemma('plan.n.01.program'), Lemma('plan.n.01.programme')]\n"
    }
   ],
   "source": [
    "print(syns[0].lemmas()) # These are synonyms for a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "plan\n"
    }
   ],
   "source": [
    "print(syns[0].lemmas()[0].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "a series of steps to be carried out or goals to be accomplished\n"
    }
   ],
   "source": [
    "print(syns[0].definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['they drew up a six-step plan', 'they discussed plans for a new bond issue']\n"
    }
   ],
   "source": [
    "print(syns[0].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'well', 'respectable', 'dependable', 'right', 'soundly', 'adept', 'goodness', 'unspoiled', 'proficient', 'trade_good', 'honorable', 'expert', 'effective', 'estimable', 'ripe', 'beneficial', 'in_force', 'serious', 'skilful', 'near', 'practiced', 'salutary', 'in_effect', 'honest', 'secure', 'skillful', 'upright', 'commodity', 'unspoilt', 'just', 'undecomposed', 'sound', 'thoroughly', 'dear', 'safe', 'full', 'good'}\n\n\n{'evilness', 'evil', 'badness', 'ill', 'bad'}\n"
    }
   ],
   "source": [
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(\"good\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "\n",
    "print(set(synonyms))\n",
    "print(\"\\n\")\n",
    "print(set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.9090909090909091\n"
    }
   ],
   "source": [
    "w1 = wordnet.synset(\"ship.n.01\") # n -> noun and 01 -> first one\n",
    "w2 = wordnet.synset(\"boat.n.01\")\n",
    "\n",
    "print(w1.wup_similarity(w2)) # Compare similarity of word 1 and word 2, answer is in *100 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.6956521739130435\n"
    }
   ],
   "source": [
    "w1 = wordnet.synset(\"ship.n.01\")\n",
    "w2 = wordnet.synset(\"car.n.01\")\n",
    "\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.32\n"
    }
   ],
   "source": [
    "w1 = wordnet.synset(\"ship.n.01\")\n",
    "w2 = wordnet.synset(\"cat.n.01\")\n",
    "\n",
    "print(w1.wup_similarity(w2))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bit054245dcf21e4c25b7b8d8cc669b6593",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}